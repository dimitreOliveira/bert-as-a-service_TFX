<img src="https://github.com/dimitreOliveira/bert-as-a-service_TFX/blob/main/Assets/colab_icon.png?raw=true" width="600" height="200">

### Google Colab pipeline

**BERT from TF HUB**
- Model: BERT base uncased (english)
- Data: IMDB movie review (5,000 samples)
- Pre-processing: Text trimming, Tokenizer (sequence length of 128, lower case)
- Training: `epochs`: 3, `batch size`: 32, `learning rate`: 1e-5, `loss`: binary crossentropy.

**BERT from HuggingFace**
- Model: BERT base uncased (english)
- Data: IMDB movie review (5,000 samples)
- Pre-processing: Text trimming, Tokenizer (sequence length of 128, lower case)
- Training: `epochs`: 3, `batch size`: 32, `learning rate`: 1e-5, `loss`: binary crossentropy.
